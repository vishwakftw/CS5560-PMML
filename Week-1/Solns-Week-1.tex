\documentclass{article}
\usepackage[margin=0.6in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}

\title{Solutions to the Assignment - 1 : CS5560 - \\
Probabilistic Models in Machine Learning}
\author{Vishwak Srinivasan\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Exercises from ML: A Probabilistic Perspective}
\subsection*{Exercise 2.1}
\begin{flushleft}
There are four events to consider: BB, BG, GB and GG (B = Boy, G = Girl), and the probabilities associated with each event is \(\frac{1}{4}\).
\subsubsection*{Part a}
Since there is at least one boy, through counting (BB, BG, GB): the probability of the other one child being a girl is \(\frac{2}{3}\).

\subsubsection*{Part b}
Now, we require the probability \(P(\text{other is a girl} | \text{there is definitely a boy})\). For ease, \(O_{G}\) = other is a girl and \(B\) = boy. These events are independent of each other just like successive coin tosses, and hence: \(P(O_{G} | B) = P(O_{G}) = \frac{1}{2}\).
\end{flushleft}

\subsection*{Exercise 2.4}
\begin{flushleft}
Stating the details: \(+\) = positve test, \(-\) = negative test, \(D\) = diseased, \(ND\) = healthy.
\begin{center}
\begin{tabular}{cc}
\(P(+ | D) = 0.99\) & \(P(- | ND) = 0.99\) \\
\(P(+ | ND) = 0.01\) & \(P(- | D) = 0.01\) \\
\(P(D) = 0.0001\) & \(P(ND) = 0.999\)
\end{tabular}
\end{center}
We want: \(P(D | +)\). By Bayes' Rule:
\begin{multline}
P(D | +) = \frac{P(+ | D) \times P(D)}{P(+)} = \frac{P(+ | D) \cdot P(D)}{P(+ | D) \cdot P(D) + P(+ | ND) \cdot P(ND)} = \frac{0.99 \cdot 0.0001}{0.99 \cdot 0.0001 + 0.01 \cdot 0.9999}\\ \approx 0.009804
\end{multline}
\end{flushleft}

\subsection*{Exercise 2.7}
\begin{flushleft}
Contradicting via example: consider 4 objects, which can be picked equiprobably. Define three events, where the first object is picked in all three events along with another object. That is, \(E_{1} = \{\text{first}, \text{second}\}\), \(E_{2} = \{\text{first}, \text{third}\}\) and \(E_{3} = \{\text{first}, \text{fourth}\}\).
\begin{center}
\begin{tabular}{ccc}
& \(P(E_{i}) = \frac{1}{2}\) \(\forall i \in \{1, 2, 3\}\) & \\
\(P(E_{1} \cup E_{2}) = \frac{1}{4} = P(E_{1}) P(E_{2})\) & \(P(E_{1} \cup E_{3}) = \frac{1}{4} = P(E_{1}) P(E_{3})\) & \(P(E_{2} \cup E_{3}) = \frac{1}{4} = P(E_{2}) P(E_{3})\)
\end{tabular}
\end{center}
Now \(P(E_{1} \cup E_{2} \cup E_{3}) = \frac{1}{4} \neq P(E_{1}) \cdot P(E_{2}) \cdot P(E_{3}) = \frac{1}{8}\). Hence, pairwise independence doesn't necessarily imply mutual independence.
\end{flushleft}

\subsection*{Exercise 2.10}
\begin{flushleft}
\begin{equation}
P(Y \leq y) = P\left(\frac{1}{X} \leq y\right) = P\left(X \geq \frac{1}{y}\right) = 1 - P\left(X \leq \frac{1}{y}\right)
\end{equation}
Now, since \(f_{Y}(x) = \frac{\partial P(Y \leq x)}{\partial x}\):
\begin{equation}
f_{Y}(x) = \frac{\partial P(Y \leq x)}{\partial x} = -\frac{\partial P(X \leq \frac{1}{x})}{\partial x} \cdot -\frac{1}{x^2} = \frac{f_{X}\left(\frac{1}{x}\right)}{x^2} = \frac{b^{a}}{\Gamma(a)} \frac{x^{-a + 1} e^{-\frac{b}{x}}}{x^2} = \frac{b^{a}}{\Gamma(a)} x^{-(a + 1)} e^{-\frac{b}{x}}
\end{equation}
\end{flushleft}

\subsection*{Exercise 2.16}
\begin{flushleft}
Define the beta function to be:
\begin{equation}
\beta(a, b) = \int_{0}^{1} x^{a - 1} (1 - x)^{b - 1} dx = \frac{\Gamma(a) \Gamma(b)}{\Gamma(a + b)}
\end{equation}
Now, by the definition of the mean:
\begin{equation}
\mathbb{E}_{X \sim \text{Beta}(a, b)}[X] = \frac{1}{\beta(a, b)}\int_{0}^{1} x^{a} (1 - x)^{b - 1} dx = \frac{\beta(a + 1, b)}{\beta(a, b)} = \frac{\Gamma(a + 1) \cdot \Gamma(a + b)}{\Gamma(a) \cdot \Gamma(a + b + 1)} = \frac{a}{a + b}
\end{equation}

Similarly, the second moment of the Beta distribution is:
\begin{equation}
\mathbb{E}_{X \sim \text{Beta}(a, b)}[X^2] = \frac{1}{\beta(a, b)}\int_{0}^{1} x^{a + 1} (1 - x)^{b - 1} dx = \frac{\Gamma(a + 2) \cdot \Gamma(a + b)}{\Gamma(a) \cdot \Gamma(a + b + 2)} = \frac{(a + 1)(a)}{(a + b)^2 + (a + b)}
\end{equation}

Using the definition of \(\text{Var}[X] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\), we get:
\begin{equation}
\text{Var}[X] = \frac{(a + 1)(a)}{(a + b)^2 + (a + b)} - \left(\frac{a}{a + b}\right)^2 = \frac{a}{a + b}\left(\frac{b}{(a + b + 1)(a + b)}\right)
\end{equation}

The mode is the solution to the expression \(\mathrm{argmax}(f_{X}(x))\). For \(a, b > 1\), this value can be found be differentiating the \emph{pdf} and setting it to zero.
\begin{equation}
\frac{1}{\beta(a, b)}\frac{\partial x^{a - 1} (1 - x)^{b - 1}}{\partial x} = (a - 1) x^{a - 2} - (b - 1) x^{b - 2} = 0 \Rightarrow x = \frac{a - 1}{a + b - 2}
\end{equation}

For \(a, b = 1\), the distribution is uniform, in which case the mode could be any value in the support of the Beta distribution which is \((0, 1)\). For \(a = 1, b > 1\), the \emph{pdf} is monotonically decreasing, and hence the mode is \(x = 0\). Similarly, for \(a > 1, b = 1\), the \emph{pdf} is monotonically increasing, and the hence the mode is \(x = 1\).
\end{flushleft}

\subsection*{Exercise 2.17}
\begin{flushleft}
First we have to construct the \emph{cdf}:
\begin{equation}
P(\min (X, Y) \leq z) = 1 - P(\min(X, Y) \geq z) = 1 - P(X \geq z) \cdot P(Y \geq z) = 1 - (1 - P(X \leq z)) \cdot (1 - P(Y \leq z))
\end{equation}
Since \(X, Y \sim \mathcal{U}(0, 1)\), \(P(X \leq z), P(Y \leq z) = z (z \in [0, 1])\). This means that:
\begin{equation}
P(\min (X, Y) \leq z) = 1 - (1 - z)^2 \Rightarrow f(z) = 2 - 2z, (z \in [0, 1])
\end{equation}

The expected position of the leftmost point would hence be:
\begin{equation}
\int_{0}^{1} zf(z) dz = \int_{0}^{1} (2z - 2z^{2}) dz = 1 - \frac{2}{3} = \frac{1}{3}
\end{equation}
\end{flushleft}

\section*{Exercises from Introduction to Probability Models}
\subsection*{Exercise 27}
\begin{flushleft}
Let the number of common heads obtained by \(X\) be \(n_{h}(X)\). Now the probability of A and B tossing the same number of heads is:
\begin{multline}
P(n_{h}(A) = n_{h}(B)) = P(\text{A tosses 1 head, B tosses 1 head}) + P(\text{A tosses 2 heads, B tosses 2 heads})\\+ \ldots + P(\text{A tosses } \min\{k, n-k\} \text{ heads, B tosses } \min\{k, n-k\} \text{ heads})
\end{multline}
Note that the events of A tossing and B tossing are independent, and hence \(P(\text{A tosses } i \text{ heads}, \text{B tosses } i \text{ heads}) = P(\text{A tosses } i \text{ heads}) \cdot P(\text{B tosses } i \text{ heads})\).
Two cases arise: \(\min\{k, n-k\} = k\) and \(\min\{k, n-k\} = n - k\).
\begin{enumerate}
\item \(\min\{k, n-k\} = k\)
\begin{multline}
\label{c1-27}
P(n_{h}(A) = n_{h}(B)) = \displaystyle \sum_{i=0}^{k} \underbrace{\binom{k}{i} \left(\frac{1}{2}\right)^{i} \left(\frac{1}{2}\right)^{k - i}}_{\text{Probability of getting } i \text{ heads from } k \text{ tosses}} \cdot \underbrace{\binom{n-k}{i} \left(\frac{1}{2}\right)^{i} \left(\frac{1}{2}\right)^{(n-k)-i}}_{\text{Probability of getting } i \text{ heads from } n-k \text{ tosses}}\\= \left(\frac{1}{2}\right)^{n} \sum_{i=0}^{k} \binom{n - k}{i} \binom{k}{k-i} = \left(\frac{1}{2}\right)^{n} \binom{n}{k}
\end{multline}

\item \(\min\{k, n-k\} = n-k\)
\begin{multline}
\label{c2-27}
P(n_{h}(A) = n_{h}(B)) = \displaystyle \sum_{i=0}^{n-k} \underbrace{\binom{k}{i} \left(\frac{1}{2}\right)^{i} \left(\frac{1}{2}\right)^{k - i}}_{\text{Probability of getting } i \text{ heads from } k \text{ tosses}} \cdot \underbrace{\binom{n-k}{i} \left(\frac{1}{2}\right)^{i} \left(\frac{1}{2}\right)^{(n-k)-i}}_{\text{Probability of getting } i \text{ heads from } n-k \text{ tosses}}\\= \left(\frac{1}{2}\right)^{n} \sum_{i=0}^{k} \binom{k}{i} \binom{n-k}{(n-k)-i} = \left(\frac{1}{2}\right)^{n} \binom{n}{k}
\end{multline}
\end{enumerate}
The last step in both the cases arise from the Vandermonde Convolution Identity\footnote{\url{https://brilliant.org/wiki/vandermondes-identity/ }} for binomial coefficients.

Now the probability of having a total of \(k\) heads out of \(n\) tosses, is simply: \(\binom{n}{k} \left(\frac{1}{2}\right)^{k} \left(\frac{1}{2}\right)^{n-k} = \binom{n}{k}\left(\frac{1}{2}\right)^{n}\) which is the same obtained in Eqns. \ref{c1-27} and \ref{c2-27}.
\end{flushleft}

\subsection*{Exercise 37}
\begin{flushleft}
The \emph{cdf} of the uniform distribution is \(F(x) = P(X \leq x) = x\) when \(x \in [0, 1]\). We also know that if \(A\) and \(B\) are two independent random variables, then \(P(A \leq a, B \leq b) = P(A \leq a) P(B \leq b)\). Using the same logic as in Problem 2.17 in the previous section, we know that:
\begin{equation}
P(M = \max_{i}\{X_{i}\}_{i=1}^{n} \leq x) = P(X_{1} \leq x, X_{2} \leq x, \ldots, X_{n} \leq x) = \displaystyle \prod_{i=1}^{n} P(X_{i} \leq x) = x^{n} \quad (x \in [0, 1])
\end{equation}
The \emph{pdf} of \(M\) is \(\frac{\partial P(M \leq x)}{\partial x} = nx^{n-1} \quad (x \in [0, 1])\).
\end{flushleft}

\subsection*{Exercise 38}
\begin{flushleft}
To be a valid density function, these conditions should hold: \(f(x) \geq 0 \quad \forall x \in (-\infty, \infty)\) and \(\displaystyle \int_{-\infty}^{\infty} f(x) = 1\).

\begin{equation}
\displaystyle \int_{0}^{\infty} ce^{-2x} dx = \frac{c}{2} = 1 \Rightarrow c = 2
\end{equation}
The first condition holds for this choice of \(c\), hence \(f(x)\) is a valid \emph{pdf}.

\begin{equation}
P(X > 2) = 1 - P(X \leq 2) = 1 - \int_{0}^{2} 2e^{-2x} dx = 1 - (1 - e^{-4}) = e^{-4}
\end{equation}
or,
\begin{equation}
P(X > 2) = \int_{2}^{\infty} 2e^{-2x} dx = e^{-4}
\end{equation}
\end{flushleft}
\end{document}
