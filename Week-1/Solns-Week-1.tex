\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Solutions to the Assignment - 1 : CS5560 - \\
Probabilistic Models in Machine Learning}
\author{Vishwak Srinivasan\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Exercises from ML: A Probabilistic Perspective}
\subsection*{Exercise 2.1}
\begin{flushleft}
There are four events to consider: BB, BG, GB and GG (B = Boy, G = Girl), and the probabilities associated with each event is \(\frac{1}{4}\).
\subsubsection*{Part a}
Since there is at least one boy, through counting (BB, BG, GB): the probability of the other one child being a girl is \(\frac{2}{3}\).

\subsubsection*{Part b}
Now, we require the probability \(P(\text{other is a girl} | \text{there is definitely a boy})\). For ease, \(O_{G}\) = other is a girl and \(B\) = boy. These events are independent of each other just like successive coin tosses, and hence: \(P(O_{G} | B) = P(O_{G}) = \frac{1}{2}\).
\end{flushleft}

\subsection*{Exercise 2.4}
\begin{flushleft}
Stating the details: \(+\) = positve test, \(-\) = negative test, \(D\) = diseased, \(ND\) = healthy.
\begin{center}
\begin{tabular}{cc}
\(P(+ | D) = 0.99\) & \(P(- | ND) = 0.99\) \\
\(P(+ | ND) = 0.01\) & \(P(- | D) = 0.01\) \\
\(P(D) = 0.0001\) & \(P(ND) = 0.999\)
\end{tabular}
\end{center}
We want: \(P(D | +)\). By Bayes' Rule:
\begin{multline}
P(D | +) = \frac{P(+ | D) \times P(D)}{P(+)} = \frac{P(+ | D) \cdot P(D)}{P(+ | D) \cdot P(D) + P(+ | ND) \cdot P(ND)} = \frac{0.99 \cdot 0.0001}{0.99 \cdot 0.0001 + 0.01 \cdot 0.9999}\\ \approx 0.009804
\end{multline}
\end{flushleft}

\subsection*{Exercise 2.7}
\begin{flushleft}
Contradicting via example: consider 4 objects, which can be picked equiprobably. Define three events, where the first object is picked in all three events along with another object. That is, \(E_{1} = \{\text{first}, \text{second}\}\), \(E_{2} = \{\text{first}, \text{third}\}\) and \(E_{3} = \{\text{first}, \text{fourth}\}\).
\begin{center}
\begin{tabular}{ccc}
& \(P(E_{i}) = \frac{1}{2}\) \(\forall i \in \{1, 2, 3\}\) & \\
\(P(E_{1} \cup E_{2}) = \frac{1}{4} = P(E_{1}) P(E_{2})\) & \(P(E_{1} \cup E_{3}) = \frac{1}{4} = P(E_{1}) P(E_{3})\) & \(P(E_{2} \cup E_{3}) = \frac{1}{4} = P(E_{2}) P(E_{3})\)
\end{tabular}
\end{center}
Now \(P(E_{1} \cup E_{2} \cup E_{3}) = \frac{1}{4} \neq P(E_{1}) \cdot P(E_{2}) \cdot P(E_{3}) = \frac{1}{8}\). Hence, pairwise independence doesn't necessarily imply mutual independence.
\end{flushleft}

\subsection*{Exercise 2.10}
\begin{flushleft}
\begin{equation}
P(Y \leq y) = P\left(\frac{1}{X} \leq y\right) = P\left(X \geq \frac{1}{y}\right) = 1 - P\left(X \leq \frac{1}{y}\right)
\end{equation}
Now, since \(f_{Y}(x) = \frac{\partial P(Y \leq x)}{\partial x}\):
\begin{equation}
f_{Y}(x) = \frac{\partial P(Y \leq x)}{\partial x} = -\frac{\partial P(X \leq \frac{1}{x})}{\partial x} \cdot -\frac{1}{x^2} = \frac{f_{X}\left(\frac{1}{x}\right)}{x^2} = \frac{b^{a}}{\Gamma(a)} \frac{x^{-a + 1} e^{-\frac{b}{x}}}{x^2} = \frac{b^{a}}{\Gamma(a)} x^{-(a + 1)} e^{-\frac{b}{x}}
\end{equation}
\end{flushleft}

\subsection*{Exercise 2.16}
\begin{flushleft}
Define the beta function to be:
\begin{equation}
\beta(a, b) = \int_{0}^{1} x^{a - 1} (1 - x)^{b - 1} dx = \frac{\Gamma(a) \Gamma(b)}{\Gamma(a + b)}
\end{equation}
Now, by the definition of the mean:
\begin{equation}
\mathbb{E}_{X \sim \text{Beta}(a, b)}[X] = \frac{1}{\beta(a, b)}\int_{0}^{1} x^{a} (1 - x)^{b - 1} dx = \frac{\beta(a + 1, b)}{\beta(a, b)} = \frac{\Gamma(a + 1) \cdot \Gamma(a + b)}{\Gamma(a) \cdot \Gamma(a + b + 1)} = \frac{a}{a + b}
\end{equation}

Similarly, the second moment of the Beta distribution is:
\begin{equation}
\mathbb{E}_{X \sim \text{Beta}(a, b)}[X^2] = \frac{1}{\beta(a, b)}\int_{0}^{1} x^{a + 1} (1 - x)^{b - 1} dx = \frac{\Gamma(a + 2) \cdot \Gamma(a + b)}{\Gamma(a) \cdot \Gamma(a + b + 2)} = \frac{(a + 1)(a)}{(a + b)^2 + (a + b)}
\end{equation}

Using the definition of \(\text{Var}[X] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\), we get:
\begin{equation}
\text{Var}[X] = \frac{(a + 1)(a)}{(a + b)^2 + (a + b)} - \left(\frac{a}{a + b}\right)^2 = \frac{a}{a + b}\left(\frac{b}{(a + b + 1)(a + b)}\right)
\end{equation}

The mode is the solution to the expression \(\mathrm{argmax}(f_{X}(x))\). For \(a, b > 1\), this value can be found be differentiating the \emph{pdf} and setting it to zero.
\begin{equation}
\frac{1}{\beta(a, b)}\frac{\partial x^{a - 1} (1 - x)^{b - 1}}{\partial x} = (a - 1) x^{a - 2} - (b - 1) x^{b - 2} = 0 \Rightarrow x = \frac{a - 1}{a + b - 2}
\end{equation}

For \(a, b = 1\), the distribution is uniform, in which case the mode could be any value in the support of the Beta distribution which is \((0, 1)\). For \(a = 1, b > 1\), the \emph{pdf} is monotonically decreasing, and hence the mode is \(x = 0\). Similarly, for \(a > 1, b = 1\), the \emph{pdf} is monotonically increasing, and the hence the mode is \(x = 1\).
\end{flushleft}

\subsection*{Exercise 2.17}
\begin{flushleft}
First we have to construct the \emph{cdf}:
\begin{equation}
P(\min (X, Y) \leq z) = 1 - P(\min(X, Y) \geq z) = 1 - P(X \geq z) \cdot P(Y \geq z) = 1 - (1 - P(X \leq z)) \cdot (1 - P(Y \leq z))
\end{equation}
Since \(X, Y \sim \mathcal{U}(0, 1)\), \(P(X \leq z), P(Y \leq z) = z (z \in [0, 1])\). This means that:
\begin{equation}
P(\min (X, Y) \leq z) = 1 - (1 - z)^2 \Rightarrow f(z) = 2 - 2z, (z \in [0, 1])
\end{equation}

The expected position of the leftmost point would hence be:
\begin{equation}
\int_{0}^{1} zf(z) dz = \int_{0}^{1} (2z - 2z^{2}) dz = 1 - \frac{2}{3} = \frac{1}{3}
\end{equation}
\end{flushleft}
\end{document}
