\documentclass{article}
\usepackage[margin=0.6in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bookmark}

\newcommand{\vct}[1]{\mathbf{#1}}

\title{Solutions to the Assignment - 4 : CS5560 - \\
Probabilistic Models in Machine Learning}
\author{Vishwak Srinivasan\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Exercises from ML: A Probabilistic Perspective}
\subsection*{Exercise 4.2}
\subsubsection*{Part a}
\begin{flushleft}
Note that \(P(Y \leq y) = P(WX \leq y)\). Now by total probability, we have:
\begin{multline}
P(Y \leq y) = P(WX \leq y) = P(WX \leq y | W = -1)P(W = -1) + P(WX \leq y | W = 1)P(W = 1)\\= P(-X \leq y)P(W = -1) + P(X \leq y)P(W = 1)
\end{multline}

This means that:
\begin{equation}
\Rightarrow P(Y \leq y) = \frac{P(-X \leq y)}{2} + \frac{P(X \leq y)}{2} = P(X \leq y)
\end{equation}
The final step was due to the fact that \(X\) is \(\mathcal{N}(0, 1)\), meaning it is symmetric about 0. This in turn means that \(X\) and \(-X\) are distributed in the same manner.

\[P(Y \leq y) = P(X \leq y) \Rightarrow Y \sim \mathcal{N}(0, 1)\]
\end{flushleft}

\subsubsection*{Part b}
\begin{flushleft}
Note that \(\mathbb{E}[X] = \mathbb{E}[Y] = 0\) since \(X, Y \sim \mathcal{N}(0, 1)\). Hence, we need to show that \(\mathbb{E}[XY] = 0\). Note that \(\mathbb{E}[XWX] = \mathbb{E}[X^2W] = \mathbb{E}[\mathbb{E}[X^2W | W]]\). Now:
\begin{equation}
\mathbb{E}[\mathbb{E}[X^2W | W]] = \mathbb{E}[X^2] \frac{1}{2} + \mathbb{E}[-X^2] \frac{1}{2} = 0
\end{equation}
This shows that \(\mathrm{cov}(X, Y) = 0\).
\end{flushleft}

\subsection*{Exercise 7.3}
\begin{flushleft}
We will first expand the form of \(J(\mathbf{w}, w_{0})\).
\begin{equation}
J(\mathbf{w}, w_{0}) = \vct{y}^{T}\vct{y} + w_{0}^{2}\vct{1}^{1} + \vct{w}^{T}\vct{X}^{T}X\vct{w} - 2\vct{y}^{T}\vct{X}\vct{w} - 2w_{0}\vct{y}^{T}\vct{1} + 2w_{0}\vct{w}^{T}\vct{X}^{T}\vct{1} + \lambda \vct{w}^{T}\vct{w}
\end{equation}

Now take the derivative of \(J\) w.r.t. \(\vct{w}\) and set to \(\vct{0}\).
\begin{gather}
\nabla_{\vct{w}} J(\vct{w}, w_{0}) = 2\vct{X}^{T}\vct{X}\vct{w} - 2\vct{X}^{T}\vct{y} + 2\vct{X}^{T}\vct{1} + 2\lambda\mathbb{I}\vct{w} = \vct{0} \\
\Rightarrow (\lambda \mathbb{I} + \vct{X}^{T}\vct{X})\vct{w} = \vct{X}^{T}\vct{y} - \vct{X}^{T}\vct{1}
\end{gather}

Note that \(\vct{X}^{T}\vct{1}\) is a column vector with the \(i^{th}\) element equal to the sum of elements of the \(i^{th}\) row of \(\vct{X}^{T}\). The \(i^{th}\) row of \(\vct{X}^{T}\) is the \(i^{th}\) column of \(\vct{X}\). The sum of elements of \(i^{th}\) column of \(\vct{X}\) is the sum of the \(i^{th}\) components of the input vectors. Note that the input is centered, and hence this sum is \(0\) because the mean is \(0\). Due to this:
\begin{equation}
\vct{w}^{*} = (\vct{X}^{T}\vct{X} + \lambda \mathbb{I})^{-1} \vct{X}^{T}\vct{y}
\end{equation}

Similarly, we calculate the derivative of \(J(\vct{w}, w_{0})\) w.r.t. \(w_{0}\).
\begin{gather}
\frac{\partial J(\vct{w}, w_{0})}{\partial w_{0}} = 2w_{0}\vct{1}^{T}\vct{1} - 2\vct{y}^{T}\vct{1} + \vct{w}^{T}\vct{X}^{T}\vct{1} = 2w_{0}\vct{1}^{T}\vct{1} - \displaystyle 2\sum_{i=1}^{m}y_{i} = 0\\
w_{0}^{*} = \frac{\displaystyle 2\sum_{i=1}^{m}y_{i}}{\vct{1}^{T}\vct{1}} = \overline{\vct{y}}
\end{gather}

The final step is due to the fact that \(\vct{1}^{T}\vct{1} = m\).
\end{flushleft}

\subsection*{Exercise 7.4}
\begin{flushleft}
The conditional log-likelihood in linear regression is:
\begin{equation}
\log p(y_{i} | \vct{x}_{i}, \hat{\vct{w}}, \sigma^{2}) = -\frac{1}{2}\log \sigma^{2} - \log 2\pi - \frac{1}{2}\left(\frac{(y_{i} - \hat{\vct{w}}^{T}\vct{x}_{i})^{2}}{\sigma^{2}}\right)
\end{equation}

Now the conditional log-likelihood over the entire dataset is:
\begin{equation}
L(\sigma^{2}) = \log p(\vct{y} | \vct{X}, \hat{\vct{w}}, \sigma^{2}) = -\frac{N}{2}\log\sigma^{2} - N\log 2\pi - \frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(y_{i} - \hat{\vct{w}}^{T}\vct{x}_{i})^{2}
\end{equation}

Taking the derivative of \(L(\sigma^{2})\) w.r.t. \(\sigma^{2}\) and setting to 0 gives us:
\begin{gather}
\frac{\partial L(\sigma^{2})}{\partial \sigma^{2}} = -\frac{N}{2\sigma^{2}} + \frac{1}{2(\sigma^{2})^{2}}\left(\sum_{i=1}^{N}(y_{i} - \hat{\vct{w}}^{T}\vct{x}_{i})^{2}\right) = 0\\
\Rightarrow \hat{\sigma^{2}} = \frac{1}{N}\sum_{i=1}^{N}(y_{i} - \hat{\vct{w}}^{T}\vct{x}_{i})^{2}
\end{gather}
\end{flushleft}

\subsection*{Exercise 7.5}
\begin{flushleft}
We re-trace the steps in Bayesian Linear Regression. We need to minimize the expected KL-divergence between \(P^{*}(y | \vct{x})\) and \(P_{\theta}(y | \vct{x})\). By total expectation rule, this can be converted to an optimization problem as shown below:
\begin{equation}
-\max_{\vct{w}, w_{0}} E_{\vct{x}, y \sim P^{*}(\vct{x}, y)} \log p_{\vct{w}, w_0}(y | \vct{x})
\end{equation}

We will approximate the expectation with a summation over the dataset, and consider a scalar-valued Gaussian (as in the question) with constant variance \(= \sigma^{2}\). Removing terms irrelevant to the optimization problem, we have:
\begin{equation}
-\max -\frac{1}{2\sigma^2} \sum_{i=1}^{N} (y_{i} - \vct{w}^{T}\vct{x}_{i} - w_{0})^{2} = \min \frac{1}{2\sigma^2} \sum_{i=1}^{N} \left((y_{i} - \vct{w}^{T}\vct{x}_{i})^{2} + w_{0}^{2} - 2(y_{i} - \vct{w}^{T}\vct{x}_{i})w_{0}\right)
\end{equation}

Since this is an unconstrained optimization problem, we can directly take the partial derivative / gradients w.r.t. to the parameters to obtain the estimate. First, we take the partial derivative of the objective w.r.t. \(w_{0}\) and set it to 0:
\begin{equation}
2Nw_{0} - 2\sum_{i=1}^{N}(y_{i} - \vct{w}^{T}\vct{x}_{i}) = 0 \Rightarrow \hat{w_{0}} = \frac{1}{N}\sum_{i=1}^{N}y_{i} - \frac{1}{N}\sum_{i=1}^{N}\vct{x}_{i}^{T}\vct{w} = \overline{y} - \overline{\vct{x}}^{T}\vct{w}
\end{equation}

Now take the gradient of \(\vct{w}\) and set it to 0. Before that, we will move some terms around for easier calculation:
\begin{multline}
\min \sum_{i=1}^{N} \left((y_{i} - \overline{y}) - (\vct{w}^{T}\vct{x_{i}} -\vct{w}^{T}\overline{\vct{x}})\right)^{2} = \min \sum_{i=1}^{N} (\vct{w}^{T}(\vct{x}_{i} - \overline{\vct{x}}))^{2} - 2\vct{w}\sum_{i=1}^{N} (y_{i} - \overline{y})(\vct{x}_{i} - \overline{\vct{x}}) \\
\Rightarrow \nabla_{\vct{w}} \text{objective} = 0 \Rightarrow \sum_{i=1}^{N} (\vct{x}_{i} - \overline{\vct{x}})(\vct{x}_{i} - \overline{\vct{x}})^{T}\vct{w} = \sum_{i=1}^{N}(y_{i} - \overline{y})(\vct{x}_{i} - \overline{\vct{x}}) \Rightarrow \vct{X_{c}}^{T}\vct{X_{c}}w = \vct{X_{c}}^{T}\vct{y}_{c}
\end{multline}

This means that: \(\hat{\vct{w}} = \left(\vct{X_{c}}^{T}\vct{X_{c}}\right)^{-1}\vct{X_{c}}^{T}\vct{y}_{c}\).
\end{flushleft}

\subsection*{Exercise 7.6}
\begin{flushleft}
Using the solution in the previous exercise for \(D = 1\), we have:
\begin{equation}
\hat{\vct{w}} = w_{1} = \frac{\sum_{i=1}^{N} (x_{i} - \overline{x})(y_{i} - \overline{y})}{\sum_{i=1}^{N} (x_{i} - \overline{x})^{2}} = \frac{\sum_{i=1}^{N} (x_{i}y_{i}) - \overline{y}\sum_{i=1}^{N}(x_{i} - \overline{x}) - \overline{x}\sum_{i=1}^{N}(y_{i} - \overline{y}) + N\overline{x}\overline{y}}{\sum_{i=1}^{N}(x_{i} - \overline{x})^{2}} = \frac{\sum_{i=1}^{N} x_{i}y_{i} - N\overline{x}\overline{y}}{\sum_{i=1}^{N} (x_{i} - \overline{x})^{2}}
\end{equation}

\begin{equation}
w_{0} = \overline{y} - w_{1}\overline{x} 
\end{equation}
\end{flushleft}
\end{document}
