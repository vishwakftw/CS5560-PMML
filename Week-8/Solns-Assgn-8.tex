\documentclass{article}
\usepackage[margin=0.6in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bookmark}
\usepackage{graphicx}
\usepackage{float}

\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}

\title{Solutions to the Assignment - 8 : CS5560 - \\
Probabilistic Models in Machine Learning}
\author{Vishwak Srinivasan\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Exercises from ML: A Probabilistic Perspective}
\subsection*{Exercise 5.8}
\subsubsection*{Part a}
We use the formula : \(p(x, y | \theta) = p(y | x, \theta_{2}) p(x | \theta_{1})\).

\begin{center}
\begin{tabular}{c|cc}
\(p(x, y | \theta)\) & \(y = 0\) & \(y = 1\) \\
\hline
\(x = 0\) & \(\theta_{2}(1 - \theta_{1})\) & \((1 - \theta_{2})(1 - \theta_{1})\) \\
\(x = 1\) & \((1 - \theta_{2})\theta_{1}\) & \(\theta_{2}\theta_{1}\)
\end{tabular}
\end{center}

\subsubsection*{Part b}
\begin{flushleft}
We require: \(\displaystyle \argmax_{\theta_{1}, \theta_{2} \in (0, 1)} p(\mathcal{D} | \theta_{1}, \theta_{2})\).
\begin{equation}
p(\mathcal{D} | \theta_{1}, \theta_{2}) = \prod_{i=1}^{7} p(x_{i}, y_{i} | \theta) = \prod_{i=1}^{7} p(y_{i} | x_{i}, \theta) \prod_{i=1}^{7} p(x_{i} | \theta) = \theta_{1}^{4}(1 - \theta_{1})^{3} \cdot \theta_{2}^{4}(1 - \theta_{2})^{3}
\end{equation}

The maximum is attained at: \(\hat{\theta}_{1} = \frac{4}{7}\) and \(\hat{\theta}_{2} = \frac{4}{7}\). The value of \(p(\mathcal{D} | \hat{\theta}, M_{2}) = \left(\frac{4}{7}\right)^{8} \left(\frac{3}{7}\right)^{6}\).
\end{flushleft}

\subsubsection*{Part c}
\begin{flushleft}
We require: \(\displaystyle \argmax_{\theta_{00}, \theta_{01}, \theta_{10}, \theta_{11} \in (0, 1)} p(\mathcal{D} | \theta_{00}, \theta_{01}, \theta_{10}, \theta_{11}) \).
\begin{equation}
p(\mathcal{D} | \theta_{00}, \theta_{01}, \theta_{10}, \theta_{11}) = \prod_{i=1}^{7} p(x_{i}, y_{i} | \theta) = \theta_{11}\theta_{10}\theta_{00}\theta_{10}\theta_{11}\theta_{00}\theta_{01} = \theta_{00}^{2}\theta_{01}\theta_{10}^{2}\theta_{11}^{2}
\end{equation}

Note that the objective looks exactly like a Multinoulli MLE objective, and hence the obtained the maximizers is easy: \(\hat{\theta}_{00} = \frac{2}{7}, \hat{\theta}_{01} = \frac{1}{7}, \hat{\theta}_{10} = \frac{2}{7}\) and \(\hat{\theta}_{11} = \frac{2}{7}\). The value of \(p(\mathcal{D} | \theta_{00}, \theta_{01}, \theta_{10}, \theta_{11}) = \left(\frac{2}{7}\right)^{6}\frac{1}{7}\).
\end{flushleft}

\subsubsection*{Part d}
\begin{flushleft}
From the dataset, it is easy to calculate the MLE for \(\mathcal{D}_{-i}\) for all \(i = \{1, 2, \ldots, 7\}\). These values are tabulated below for the respective models - \(M_{2}\) first and then \(M_{4}\):
\begin{center}
\begin{tabular}{c|ccccccc}
\(i\) & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
\hline
\(\theta_{1}\) & \(\frac{1}{2}\) & \(\frac{1}{2}\) & \(\frac{2}{3}\) & \(\frac{1}{2}\) & \(\frac{1}{2}\) & \(\frac{2}{3}\) & \(\frac{2}{3}\) \\
\(\theta_{2}\) & \(\frac{1}{2}\) & \(\frac{2}{3}\) & \(\frac{1}{2}\) & \(\frac{2}{3}\) & \(\frac{1}{2}\) & \(\frac{1}{2}\) & \(\frac{2}{3}\) \\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{c|ccccccc}
\(i\) & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
\hline
\(\theta_{00}\) & \(\frac{1}{3}\) & \(\frac{1}{3}\) & \(\frac{1}{6}\) & \(\frac{1}{3}\) & \(\frac{1}{3}\) & \(\frac{1}{6}\) & \(\frac{1}{3}\) \\
\(\theta_{01}\) & \(\frac{1}{6}\) & \(\frac{1}{6}\) & \(\frac{1}{6}\) & \(\frac{1}{6}\) & \(\frac{1}{6}\) & \(\frac{1}{6}\) & \(0\) \\
\(\theta_{10}\) & \(\frac{1}{3}\) & \(\frac{1}{6}\) & \(\frac{1}{3}\) & \(\frac{1}{6}\) & \(\frac{1}{3}\) & \(\frac{1}{3}\) & \(\frac{1}{3}\) \\
\(\theta_{11}\) & \(\frac{1}{6}\) & \(\frac{1}{3}\) & \(\frac{1}{3}\) & \(\frac{1}{3}\) & \(\frac{1}{6}\) & \(\frac{1}{3}\) & \(\frac{1}{3}\) \\
\end{tabular}
\end{center}

Now computing the metric \(L(M_{1})\):
\begin{equation}
L(M_{1}) = \log\frac{1}{4} + \log\frac{1}{6} + \log\frac{1}{6} + \log\frac{1}{6} + \log\frac{1}{4} + \log\frac{1}{6} + \log\frac{1}{9} = 0.245
\end{equation}

And similarly \(L(M_{2})\):
\begin{equation}
L(M_{2}) = \log\frac{1}{6} + \log\frac{1}{6} + \log\frac{1}{6} + \log\frac{1}{6} + \log\frac{1}{6} + \log\frac{1}{6} + \log 0 = -\infty
\end{equation}

LOO-CV will pick \(M_{1}\) because the likelihood is higher than that of \(M_{2}\).
\end{flushleft}

\subsection*{Exercise 6.1}
\begin{flushleft}
Since the input features are entirely random, we can resort to randomly labeling the dataset. Since there are \(N_{1}\) examples of class 1 and \(N_{2}\) examples of class 2, for a given input we can predict it to belong to the class with the most number of samples. This way, the best misclassification rate one can achieve is \(\frac{\min\{N_{1}, N_{2}\}}{N_{1} + N_{2}}\). In our case, \(N_{1} = N_{2}\), hence the best misclassification rate is \(50\%\).
\(\newline\)

Now let's use the same algorithm for LOO-CV. First let us consider \((N_{1} - 1)\) samples of class 1 and \(N_{2}\) samples of class 2. With \(N_{1} = N_{2}\), this means that class 2 is the predominant class, and hence the predicted label will be 2 for a validation data point. This will repeat \(N_{1}\) times for class 1. Similarly for class 2, the predicted label will be 1 for a validation data point, and this will repeat \(N_{2}\) times. Note that for all of the validation samples, the wrong class was predicted, which means that the LOO-CV error is \(100\%\)!.
\end{flushleft}
\end{document}
