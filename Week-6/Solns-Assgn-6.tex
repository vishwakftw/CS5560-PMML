\documentclass{article}
\usepackage[margin=0.6in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bookmark}
\usepackage{graphicx}
\usepackage{float}

\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}

\title{Solutions to the Assignment - 6 : CS5560 - \\
Probabilistic Models in Machine Learning}
\author{Vishwak Srinivasan\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Exercises from ML: A Probabilistic Perspective}
\subsection*{Exercise 3.2}
\begin{flushleft}
Note that for any \(n, m > 0\):
\begin{equation}
n \cdot (n + 1) \cdot (n + 2) \cdots (n + m) = \frac{1 \cdot 2 \cdots (n - 1) \cdot n \cdot (n + 1) \cdot (n + 2) \cdot (n + m)}{1 \cdot 2 \cdots (n - 1)} = \frac{(n + m)!}{(n - 1)!}
\end{equation}

From the above equation, in our case, we can simplify the following to:
\begin{itemize}
\item \(\alpha_{1} \cdot (\alpha_{1} + 1) \cdots (\alpha_{1} + N_{1} - 1) = \frac{(\alpha_{1} + N_{1} - 1)}{(\alpha_{1} - 1)}\)
\item \(\alpha_{0} \cdot (\alpha_{0} + 1) \cdots (\alpha_{0} + N_{0} - 1) = \frac{(\alpha_{0} + N_{0} - 1)}{(\alpha_{0} - 1)}\)
\item \(\alpha \cdot (\alpha + 1) \cdots (\alpha + N - 1) = \frac{(\alpha + N - 1)}{(\alpha - 1)}\)
\end{itemize}

Based on these equations:
\begin{equation}
\label{raw-32}
\frac{[\alpha_{1} \cdot (\alpha_{1} + 1) \cdots (\alpha_{1} + N_{1} - 1)][\alpha_{0} \cdot (\alpha_{0} + 1) \cdots (\alpha_{0} + N_{0} - 1)]}{\alpha \cdot (\alpha + 1) \cdots (\alpha + N - 1)} = \frac{(\alpha_{1} + N_{1} - 1)}{(\alpha_{1} - 1)} \times \frac{(\alpha_{0} + N_{0} - 1)}{(\alpha_{0} - 1)} \times \frac{(\alpha - 1)}{(\alpha + N - 1)}
\end{equation}

From the fact that \(\Gamma(z) = (z - 1)!\) and \(\alpha_{1} + \alpha_{0} = \alpha\), Eqn \ref{raw-32} simplifies to
\[\displaystyle \frac{\Gamma(\alpha_{1} + N_{1}) \Gamma(\alpha_{0} + N_{0}) \Gamma(\alpha_{0} + \alpha_{1})}{\Gamma(\alpha_{0} + \alpha_{1} + N) \Gamma(\alpha_{1}) \Gamma(\alpha_{0})}\]
\end{flushleft}

\subsection*{Exercise 3.4}
\begin{flushleft}
\begin{equation*}
p(\theta | X < 3) \propto p(\theta) p(X < 3 | \theta) \hfill \text{(From Bayes' Rule)}
\end{equation*}

Without loss of generality, assume that \(p(\theta) \sim \mathrm{Beta}(\alpha, \beta) \propto \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}\). Now note that:
\begin{equation}
p(X < 3 | \theta) = p(X = 0 | \theta) + p(X = 1 | \theta) + p(X = 2 | \theta) = (1 - \theta)^{5} + 5\theta(1 - \theta)^{4} + 10\theta^{2}(1 - \theta)^{3}
\end{equation}

Substuting this in the preliminary equation:
\begin{equation}
p(\theta | X < 3) \propto \theta^{\alpha - 1}(1 - \theta)^{(5 + \beta) - 1} + 5\theta^{(\alpha + 1) - 1}(1 - \theta)^{(4 + \beta) - 1} + 10\theta^{(\alpha + 2) - 1}(1 - \theta)^{(3 + \beta) - 1} \sim \text{Mixture of three Beta distributions}
\end{equation}
The three Beta distributions are (after substituting \(\alpha = 1, \beta = 1\) from the question):
\begin{itemize}
\item Beta\((1, 6)\) \item Beta\((2, 5)\) \item Beta\((3, 4)\)
\end{itemize}
Finally:
\[p(\theta | X < 3) \sim \text{Beta}(1, 6) + 5\text{Beta}(2, 5) + 10\text{Beta}(3, 4)\]
\end{flushleft}

\subsection*{Exercise 3.7}
\subsubsection*{Part a}
\begin{flushleft}
The \textit{pdf} of a Poisson distribution is given by:
\[p(x | \lambda) = \frac{\lambda^{x}\exp(-\lambda)}{x!}\]

Now with a slight abuse of notation, we can write:
\begin{equation}
p(\mathcal{D} | \lambda) = \frac{\displaystyle\prod_{i=1}^{m}\lambda^{x_{i}}e^{-\lambda}}{\displaystyle\prod_{i=1}^{m}x_{i}!} = \frac{\lambda^{\displaystyle \sum_{i=1}^{m}x_{i}}\exp(-m\lambda)}{\displaystyle\prod_{i=1}^{m}x_{i}!}
\end{equation}

For ease, consider \(\displaystyle \sum_{i=1}^{m} x_{i} = S\) and \(\displaystyle \prod_{i=1}^{m} x_{i}! = PF\).
Now we can proceed to compute \(p(\lambda | \mathcal{D})\), using the fact that \(p(\lambda) \propto \lambda^{a - 1}\exp(-b\lambda)\)
\begin{equation}
p(\lambda | \mathcal{D}) \propto p(\lambda) p(\mathcal{D} | \lambda) = \lambda^{a - 1} \lambda^{S} \exp(-b\lambda) \exp(-m\lambda) = \lambda^{(a + S) - 1} \exp(-(b + m)\lambda) \propto \text{Gamma}(a + S, b + m)
\end{equation}

The normalization constant can be easily obtained via the normalization constant of the Gamma distribution itself. The normalization constant of the Gamma distribution is given by: \(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\). Therefore the complete posterior can be written as:
\begin{equation}
p(\lambda | \mathcal{D}) = \frac{(b + m)^{(a + S)}}{\Gamma(a + S)}\lambda^{a + S - 1}\exp(-(b + m)\lambda)
\end{equation}
\end{flushleft}

\subsubsection*{Part b}
\begin{flushleft}
The posterior mean is given by:
\begin{equation}
\mu(a, b) = \frac{(a + S)}{(b + m)} \Rightarrow \lim_{a, b \to 0} \mu(a, b) = \frac{S}{m} = \frac{1}{m}\displaystyle\sum_{i=1}^{m}x_{i}
\end{equation}
\end{flushleft}

\subsection*{Exercise 3.10}
\begin{flushleft}
We require the solution of Exercise 3.9 for this problem. Thus, we find the posterior distribution:
\begin{equation}
p(\theta | \mathcal{D}) = \frac{p(\mathcal{D, \theta})}{p(\mathcal{D})} =
\begin{cases}
\frac{\frac{Kb^{K}}{\theta^{N + K + 1}}\mathbf{1}_{\theta \geq \max{D}}}{\frac{K}{(N + K)b^{N}}} = \frac{(N + K)b^{N+K}}{\theta^{N + K + 1}}& m \leq b\\
\frac{\frac{Kb^{K}}{\theta^{N + K + 1}}\mathbf{1}_{\theta \geq \max{D}}}{\frac{Kb^{K}}{(N + K)m^{N + K}}} = \frac{(N + K)m^{N + K}}{\theta^{N + K + 1}} & m > b
\end{cases} = \frac{(N + K)\max(m, b)^{N + K}}{\theta^{N + K + 1}}\mathbf{1}_{\theta \geq \max{D}}
\end{equation}
\end{flushleft}

\subsubsection*{Part a}
\begin{flushleft}
In our case, \(b, K = 0\) and \(m = 100\), \(N = 1\). Hence the posterior is given by:
\begin{equation}
p(\theta | \mathcal{D}) = \frac{1\cdot(100)^{1}}{\theta^{(1 + 1)}} = Pa(1, 100)
\end{equation}
\end{flushleft}

\subsubsection*{Part b}
\begin{flushleft}
The mean of the Pareto distribution is given by \(\frac{kB}{k - 1}\). The posterior derived has \(k = 1\), hence mean doesn't exist. The median however, can be given by \(B\sqrt[k]{2} = 100 \cdot 2 = 200\) and the mode is given by \(B = 100\).
\end{flushleft}

\subsubsection*{Part c}
\begin{flushleft}
From the equation given in the question and the posterior derived for the parameters, we can write the predictive probability as:
\begin{equation}
\label{pred-prob-310}
p(x | D, \alpha) =
\begin{cases}
\displaystyle \int_{x}^{\infty} \frac{1}{\theta} \frac{m}{\theta^{2}} d\theta & \text{ if } x > m \\
\displaystyle \int_{m}^{\infty} \frac{1}{\theta} \frac{m}{\theta^{2}} d\theta & \text{ if } x \leq m
\end{cases}
= \frac{m}{2x^2}\mathbf{1}_{x > m} + \frac{1}{2m}\mathbf{1}_{x \leq m}
\end{equation}

The limits change because of the validity of the range, as given in Equation 3.95. If \(x > m\), then the new maximum would be \(x\), and if \(x \leq m\), then the maximum value would remain unchanged.
\end{flushleft}

\subsubsection*{Part d}
\begin{flushleft}
From Eqn \ref{pred-prob-310}, we can easily write down the expressions for the predictive probability:
\begin{gather}
p(x = 100 | D, \alpha) = \frac{m}{20000}\mathbf{1}_{100 > m} + \frac{1}{2m}\mathbf{1}_{100 \leq m}\\
p(x = 50 | D, \alpha) = \frac{m}{5000}\mathbf{1}_{50 > m} + \frac{1}{2m}\mathbf{1}_{50 \leq m}\\
p(x = 150 | D, \alpha) = \frac{m}{45000}\mathbf{1}_{150 > m} + \frac{1}{2m}\mathbf{1}_{150 \leq m}
\end{gather}
\end{flushleft}

\subsubsection*{Part e}
The taxicab numbers are integral values, and hence it would benefit if we were to use a discrete distribution rather than a continuous one. The priors taken are technically incorrect, so the model would be better if the prior was a bit more informative.

\subsection*{Exercise 3.11}
\subsubsection*{Part a}
\begin{flushleft}
We consider the log-likelihood for the \(i^{th}\) sample, and collectively the log-likelihood of the dataset:
\begin{gather}
\log p_{\theta}(x_{i}) = \log \theta - \theta x_{i} \\
g(\theta) = \log p_{\theta}(\mathcal{D}) = \displaystyle \sum_{i=1}^{m} \log p_{\theta}(x_{i}) = m\log\theta - \theta \sum_{i=1}^{m} x_{i}
\end{gather}

Note that the domain is open again. Taking the derivative of \(g(\theta)\) w.r.t. \(\theta\) and setting to 0 gives us:
\begin{equation}
g'(\theta) = \frac{m}{\theta} - \sum_{i=1}^{m} x_{i} = 0 \Rightarrow \hat{\theta} = \frac{m}{\displaystyle \sum_{i=1}^{m} x_{i}}.
\end{equation}

Since the samples are positive, this is a valid estimate.
\end{flushleft}

\subsubsection*{Part b}
\begin{flushleft}
Now, we can substitute the observed values into the expression for the MLE, giving us:
\begin{equation}
\hat{\theta} = \frac{3}{5 + 6 + 4} = \frac{1}{5}
\end{equation}
\end{flushleft}

\subsubsection*{Part c}
\begin{flushleft}
Note that \(p(\theta) \propto \exp(-\theta \lambda) = \theta^{1 - 1}\exp(-\theta \lambda) \propto \text{Gamma}(1 , \lambda)\). Due to this, the mean of \(p(\theta) = \frac{1}{\lambda}\) (Since the mean of Gamma\((a, b) = \frac{a}{b}\)). In our case, \(\frac{1}{\hat{\lambda}} = \frac{1}{3} \Rightarrow \hat{\lambda} = 3\).
\end{flushleft}

\subsubsection*{Part d}
\begin{flushleft}
\begin{equation*}
p(\theta | \mathcal{D}) \propto p(\theta) p(\mathcal{D} | \theta) \propto \hat{\lambda}e^{-\theta \hat{\lambda}} \theta^{m} \exp(-\theta \sum_{i=1}^{m}x_{i}) \propto \theta^{(m + 1) - 1} \exp(-\theta\left(\hat{\lambda} + \sum_{i=1}^{m}x_{i}\right)) = \text{Gamma}(m + 1, \hat{\lambda} + \sum_{i=1}^{m}x_{i})
\end{equation*}

Using the normalization constant of the Gamma distribution, we can write the exact form of the posterior distribution:
\begin{equation}
p(\theta | \mathcal{D}) = \frac{\displaystyle \left(3 + \sum_{i=1}^{m}x_{i}\right)^{m + 1}}{\Gamma(m + 1)} \theta^{(m + 1) - 1} \exp(-\theta\left(3 + \sum_{i=1}^{m}x_{i}\right))
\end{equation}
\end{flushleft}

\subsubsection*{Part e}
Yes, because of the fact that all of them are a case of the Gamma distribution.

\subsubsection*{Part f}
Posterior mean is \(\displaystyle \frac{m + 1}{3 + \sum_{i=1}^{m}x_{i}}\).

\subsubsection*{Part g}
The MLE mean is the inverse of the mean of observations, whereas here it is a corrected form of the mean. This correction is due to the prior form of the parameters assumed. As the number of samples increase, this mean would tend to the actual mean as computed via MLE, but is conservative for smaller number of examples.

\subsection*{Exercise 3.15}
\begin{flushleft}
\[\frac{\alpha}{\alpha + \beta} = m \Rightarrow \frac{\alpha}{m} = \alpha + \beta \text{ and } \beta = \frac{\alpha(1 - m)}{m}\]

Now, in the variance equation:
\begin{equation}
\frac{\alpha \beta}{(\alpha + \beta)^{2} (\alpha + \beta + 1)} = \frac{m^3 \alpha \beta}{\alpha^{2} (\alpha + m)} = \frac{m^3 \beta}{\alpha (\alpha + m)} = \frac{m^3 (1 - m)}{m(\alpha + m)} = v
\end{equation}

From the final equation, we can see that:
\begin{equation}
\alpha = \frac{m^{2}(1 - m) - vm}{v}
\end{equation}

and hence,
\begin{equation}
\beta = \frac{m(1 - m)^{2} - v(1 - m)}{v}
\end{equation}

If \(m = 0.7, v = 0.04\), we get: \(\alpha = \frac{11.9}{4}, \beta = \frac{5.1}{4}\)
\end{flushleft}

\subsection*{Exercise 4.14}
\subsubsection*{Part a}
\begin{flushleft}
We know that:
\begin{gather}
p(\mu | \mathcal{D}) \propto p(\mathcal{D} | \mu) p(\mu | m, s^{2}) \Rightarrow \log p(\mu | \mathcal{D}) \propto \log p(\mathcal{D} | \mu) + \log p(\mu | m, s^{2})\\
\log p(x_{i} | \mu) = -\frac{(x_{i} - \mu)^{2}}{2\sigma^{2}} - \log(\sqrt{2 \pi \sigma^{2}}) \Rightarrow \log p(\mathcal{D} | \mu) = \displaystyle -\sum_{i=1}^{n} \frac{(x_{i} - \mu)^{2}}{2\sigma^{2}} - n\log(\sqrt{2\pi\sigma^{2}})\\
\log p(\mu | m, s^{2}) = -\frac{(\mu - m)^2}{2s^2} - \log(\sqrt{2\pi s^{2}})
\end{gather}

Assembling these facts appropriately we get:
\begin{equation}
\log p(\mu | \mathcal{D}) \propto -\frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (x_{i} - \mu)^{2} - n\log(\sqrt{2\pi\sigma^{2}}) - \frac{1}{2s^{2}} (\mu - m)^{2} - \log(\sqrt{2\pi s^{2}})
\end{equation}

The proportionality constant would be \(p(\mathcal{D})\) and is independent of \(\mu\), hence we can minimize the RHS of the proportional to obtain the MAP estimate:
\begin{equation}
\hat{\mu}^{MAP} = \argmax_{\mu \in \mathbb{R}} \underbrace{-\frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (x_{i} - \mu)^{2} - \frac{1}{2s^{2}} (\mu - m)^{2}}_{f(\mu)} \text{(\hfill Excluding terms not involving \(\mu\))}
\end{equation}

Take the derivative of \(f(\mu)\) and set to 0.
\begin{equation}
\label{map-estimate-414}
\frac{\partial f(\mu)}{\partial \mu} = \frac{1}{\sigma^2}\sum_{i=1}^{n}x_{i} - \frac{n\mu}{\sigma^{2}} - \frac{\mu}{s^2} + \frac{m}{s^2} = 0 \Rightarrow \hat{\mu}^{MAP} = \frac{\displaystyle \frac{1}{\sigma^{2}} \sum_{i=1}^{n} x_{i} + \frac{m}{s^2}}{\displaystyle \frac{n}{\sigma^{2}} + \frac{1}{s^2}}
\end{equation}
\end{flushleft}

\subsubsection*{Part b}
\begin{flushleft}
An alternative way to write the MAP estimate would be:
\begin{equation}
\hat{\mu}^{MAP} = \frac{\frac{1}{\sigma^2} \hat{\mu}^{MLE} + \frac{m}{s^2n}}{\frac{1}{\sigma^2} + \frac{1}{s^2n}}
\end{equation}

This can be obtained by dividing the numerator and denominator by \(n\) and \(\hat{\mu}^{MLE} = \frac{1}{n}\displaystyle\sum_{i=1}^{n}x_{i}\). We require this:
\begin{equation}
\lim_{n\to\infty} \hat{\mu}^{MAP} = \lim_{n\to\infty} \frac{\frac{1}{\sigma^2} \hat{\mu}^{MLE} + \frac{m}{s^2n}}{\frac{1}{\sigma^2} + \frac{1}{s^2n}} = \frac{\frac{\hat{\mu}^{MLE}}{\sigma^{2}}}{\frac{1}{\sigma^{2}}} = \hat{\mu}^{MLE}
\end{equation}

Hence as the number of examples increases, the MAP estimate converges to the MLE.
\end{flushleft}

\subsubsection*{Part c}
\begin{flushleft}
Using the original form of the MAP estimate in Eqn \ref{map-estimate-414}, we need to evaluate the limit as \(s^{2}\) tends to \(\infty\).
\begin{equation}
\lim_{s^{2}\to\infty} \hat{\mu}^{MAP} = \lim_{s^{2} \to \infty} \frac{\displaystyle \frac{1}{\sigma^{2}} \sum_{i=1}^{n} x_{i} + \frac{m}{s^2}}{\displaystyle \frac{n}{\sigma^{2}} + \frac{1}{s^2}} = \frac{\frac{1}{\sigma^2} \displaystyle \sum_{i=1}^{n}x_{i}}{\frac{n}{\sigma^{2}}} = \hat{\mu}^{MLE}
\end{equation}

Hence as \(s^{2}\) increases, the MAP estimate converges to the MLE.
\end{flushleft}

\subsubsection*{Part d}
\begin{flushleft}
The MAP estimate can also be written in another way:
\begin{equation}
\hat{\mu}^{MAP} = \frac{s^2\hat{\mu}^{MLE} + \frac{m\sigma^{2}}{n}}{s^{2} + \frac{\sigma^{2}}{n}}
\end{equation}

This can be obtained by multiplying \(\sigma^{2} s^2\) to the numerator and denominator. We require the limit as \(s^{2}\) tends to \(0\).
\begin{equation}
\lim_{s^{2}\to 0} \hat{\mu}^{MAP} = \lim_{s^{2}\to 0} \frac{s^2\hat{\mu}^{MLE} + \frac{m\sigma^{2}}{n}}{s^{2} + \frac{\sigma^{2}}{n}} = \frac{\frac{m\sigma^{2}}{n}}{\frac{\sigma^{2}}{n}} = m
\end{equation}

Hence as \(s^{2}\) decreases, the MAP estimate converges to the mean of the prior.
\end{flushleft}

\section*{Starred Problems}
\subsection*{Problem 1: 4.11 from ML: A Probabilistic Perspective}
\begin{flushleft}
We know that the likelihood is given by:
\begin{multline}
p(\mathcal{D} | \vct{\mu}, \Sigma) = (2\pi)^{-\frac{ND}{2}} |\Sigma|^{-\frac{N}{2}} \exp\left(-\frac{1}{2}\sum_{i=1}^{N}(\vct{x}_{i} - \vct{\mu})^{T}\Sigma^{-1}(\vct{x}_{i} - \vct{\mu})\right)\\= (2\pi)^{-\frac{ND}{2}} |\Sigma|^{-\frac{N}{2}} \exp\left(-\frac{1}{2}tr(\Sigma^{-1}S_{\bar{x}}) + N(\vct{\bar{x}} - \vct{\mu})^{T}\Sigma^{-1}(\vct{\bar{x}} - \vct{\mu})\right)
\end{multline}
The last equation comes from Eqn 4.195 in the text. Combining this with the prior, we have the posterior given by:
\begin{gather}
p(\vct{\mu}, \Sigma | \mathcal{D}) \propto |\Sigma|^{-\frac{1}{2}(\nu_{0} + N + D + 2)} \exp\left(-\frac{1}{2}\left(\underbrace{\sum_{i=1}^{N}(\vct{\bar{x}} - \vct{\mu})^{T}\Sigma^{-1}(\vct{\bar{x}} - \vct{\mu}) + \sum_{i=1}^{\kappa_{0}}(m_{0} - \vct{\mu})^{T}\Sigma^{-1}(m_{0} - \vct{\mu})}_{\text{This is a combination of two types of terms, with \(N\) and \(\kappa_{0}\) values}} + tr(\Sigma^{-1}(S_{\bar{x}} + S_0))\right)\right)\\
p(\vct{\mu}, \Sigma | \mathcal{D}) \propto |\Sigma|^{-\frac{1}{2}(\nu_{0} + N + D + 2)} \exp\left(-\frac{1}{2}\left((N + \kappa_{0})(m_{N} - \mu)^{T}\Sigma^{-1}(m_{N} - \mu) + tr(\Sigma^{-1}(S_{\bar{x}} + S_{0} + S_{m_{N}}))\right)\right)
\end{gather}

The above equation is again due to the identity in 4.195, and the similarity between the two equations. \(m_{N} = \text{Average of } N \vct{\bar{x}}\text{ s and } \kappa_{0}m_{0}\text{ s} = \frac{N\vct{\bar{x}} + \kappa_{0}m_{0}}{N + \kappa_{0}}\).

To ensure proper cancellation of terms however, one must check that:
\begin{equation}
S_{m_{N}} = N(\vct{\bar{x}} - m_{N})(\vct{\bar{x}} - m_{N})^{T} + \kappa_{0}(m_{0} - m_{N})(m_{0} - m_{N})^{T} = \frac{N\kappa_{0}}{N + \kappa_{0}}(\vct{\bar{x}} - m_{0})(\vct{\bar{x}} - m_{0})^{T}
\end{equation}

Now we can assemble and compare the terms with NIW to get the parameters of the posterior NIW:
\begin{equation}
p(\vct{\mu}, \Sigma | \mathcal{D}) \propto |\Sigma|^{-\frac{1}{2}(\nu_{0} + N + D + 2)} \exp\left(-\frac{(N + \kappa_{0})}{2}\left((\mu - m_{N})^{T}\Sigma^{-1}(\mu - m_{N}) + tr(\Sigma^{-1}(S_{\bar{x}} + S_{0} + S_{m_{N}}))\right)\right)
\end{equation}

The parameters are:
\begin{itemize}
\item \(\nu_{0} + N = \nu_{N}\)
\item \(N + \kappa_{0} = \kappa_{N}\)
\item \(S_{\bar{x}} + S_{0} + S_{m_{N}} = S_{N}\)
\item \(\frac{N\vct{\bar{x}} + \kappa_{0}m_{0}}{N + \kappa_{0}} = m_{N}\)
\end{itemize}
\end{flushleft}
\end{document}
